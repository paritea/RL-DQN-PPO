{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport gymnasium as gym\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-13T20:16:20.622378Z","iopub.execute_input":"2024-10-13T20:16:20.622754Z","iopub.status.idle":"2024-10-13T20:16:20.629290Z","shell.execute_reply.started":"2024-10-13T20:16:20.622721Z","shell.execute_reply":"2024-10-13T20:16:20.628368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install swig \n!pip install gymnasium[box2d]","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:16:20.631570Z","iopub.execute_input":"2024-10-13T20:16:20.631909Z","iopub.status.idle":"2024-10-13T20:16:43.198466Z","shell.execute_reply.started":"2024-10-13T20:16:20.631857Z","shell.execute_reply":"2024-10-13T20:16:43.197208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport sys\nfrom time import time\nfrom collections import deque, defaultdict, namedtuple\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\nfrom torch.distributions import Categorical","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:16:43.200951Z","iopub.execute_input":"2024-10-13T20:16:43.201346Z","iopub.status.idle":"2024-10-13T20:16:43.208837Z","shell.execute_reply.started":"2024-10-13T20:16:43.201311Z","shell.execute_reply":"2024-10-13T20:16:43.207928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\n    \"cuda\" if torch.cuda.is_available() else\n    \"mps\" if torch.backends.mps.is_available() else\n    \"cpu\"\n)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:16:43.210061Z","iopub.execute_input":"2024-10-13T20:16:43.210392Z","iopub.status.idle":"2024-10-13T20:16:43.219748Z","shell.execute_reply.started":"2024-10-13T20:16:43.210360Z","shell.execute_reply":"2024-10-13T20:16:43.218760Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = gym.make('LunarLander-v2')\nprint(env.action_space)\nprint(\"\")\nprint(env.observation_space)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:16:43.222023Z","iopub.execute_input":"2024-10-13T20:16:43.222789Z","iopub.status.idle":"2024-10-13T20:16:43.232169Z","shell.execute_reply.started":"2024-10-13T20:16:43.222742Z","shell.execute_reply":"2024-10-13T20:16:43.231282Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#QNetwork of my DQN implementation\nclass QNetwork(nn.Module):\n    def __init__(self, state_size, action_size, seed):\n        super(QNetwork, self).__init__()\n        self.seed = torch.manual_seed(seed)\n        self.fc1 = nn.Linear(state_size, 128)\n        self.fc2 = nn.Linear(128, 128)\n        self.fc3 = nn.Linear(128, action_size)\n    \n    def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        return self.fc3(x)\n        ","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:16:43.233274Z","iopub.execute_input":"2024-10-13T20:16:43.233599Z","iopub.status.idle":"2024-10-13T20:16:43.241835Z","shell.execute_reply.started":"2024-10-13T20:16:43.233549Z","shell.execute_reply":"2024-10-13T20:16:43.240945Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ReplayBuffer:\n    def __init__(self, buffer_size, batch_size, seed):\n        self.batch_size = batch_size\n        self.seed = random.seed(seed)\n        self.memory = deque(maxlen=buffer_size)\n        self.experience = namedtuple(\"Experience\", field_names=[\"observation\", \"action\", \"reward\", \"next_state\", \"terminated\"])\n        \n    def add(self, observation, action, reward, next_state, terminated):\n        #next_state = np.array(next_state)\n        if isinstance(observation, tuple):\n            observation = observation[0]\n        if isinstance(next_state, tuple):\n            next_state = next_state[0]\n        \n\n        experience = self.experience(observation, action, reward, next_state, terminated)\n        self.memory.append(experience)\n    \n    def sample(self):\n        experiences = random.sample(self.memory, k=self.batch_size)\n    \n        # Stacking the observations (already NumPy arrays)\n        observations = torch.from_numpy(np.vstack([experience.observation for experience in experiences])).float().to(device)\n        actions = torch.from_numpy(np.vstack([experience.action for experience in experiences]).reshape(-1, 1)).long().to(device)        \n        rewards = torch.from_numpy(np.vstack([experience.reward for experience in experiences]).reshape(-1, 1)).float().to(device)\n        next_states = torch.from_numpy(np.vstack([experience.next_state for experience in experiences])).float().to(device)\n        terminateds = torch.from_numpy(np.vstack([experience.terminated for experience in experiences]).astype(np.uint8).reshape(-1, 1)).float().to(device)\n\n        return (observations, actions, rewards, next_states, terminateds)\n    def __len__(self):\n        return len(self.memory)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:16:43.243138Z","iopub.execute_input":"2024-10-13T20:16:43.243436Z","iopub.status.idle":"2024-10-13T20:16:43.256752Z","shell.execute_reply.started":"2024-10-13T20:16:43.243405Z","shell.execute_reply":"2024-10-13T20:16:43.255786Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BUFFER_SIZE = int(1e5) # Replay memory size\nBATCH_SIZE = 64         # Number of experiences to sample from memory\nGAMMA = 0.99            # Discount factor\nTAU = 1e-3              # Soft update parameter for updating fixed q network\nLR = 1e-4               # Q Network learning rate\nUPDATE_EVERY = 4\n\nclass DQNAgent:\n    def __init__(self, state_size, action_size, seed):\n        self.state_size = state_size\n        self.action_size = action_size\n        self.seed = random.seed(seed)\n        # Initialize Q and Fixed Q networks\n        self.q_network = QNetwork(state_size, action_size, seed).to(device)\n        self.fixed_network = QNetwork(state_size, action_size, seed).to(device)\n        self.optimizer = optim.Adam(self.q_network.parameters())\n        # Initiliase memory \n        self.memory = ReplayBuffer(BUFFER_SIZE, BATCH_SIZE, seed)\n        self.timestep = 0\n        \n    \n    def step(self, observation, action, reward, next_state, terminated):\n        self.memory.add(observation, action, reward, next_state, terminated)\n        self.timestep += 1\n        if self.timestep % UPDATE_EVERY == 0:\n            if len(self.memory) > BATCH_SIZE:\n                sampled_experiences = self.memory.sample()\n                self.learn(sampled_experiences)\n        \n    def learn(self, experiences):\n        \n        states, actions, rewards, next_states, terminateds = experiences\n        action_values = self.fixed_network(next_states).detach()\n\n        max_action_values = action_values.max(1)[0].unsqueeze(1)\n        \n        Q_target = rewards + (GAMMA * max_action_values * (1 - terminateds))\n        Q_expected = self.q_network(states).gather(1, actions)\n        \n        # Calculate loss\n        loss = F.mse_loss(Q_expected, Q_target)\n        self.optimizer.zero_grad()\n        # backward pass\n        loss.backward()\n        # update weights\n        self.optimizer.step()\n        \n        # Update fixed weights\n        self.update_fixed_network(self.q_network, self.fixed_network)\n        \n    def update_fixed_network(self, q_network, fixed_network):\n        for source_parameters, target_parameters in zip(q_network.parameters(), fixed_network.parameters()):\n            target_parameters.data.copy_(TAU * source_parameters.data + (1.0 - TAU) * target_parameters.data)\n        \n        \n    def act(self, observation, eps=0.0):\n        if isinstance(observation, tuple):\n            observation = observation[0]\n        rnd = random.random()\n        if rnd < eps:\n            return np.random.randint(self.action_size)\n        else:   \n            observation = torch.from_numpy(observation).float().unsqueeze(0).to(device)\n            # set the network into evaluation mode \n            self.q_network.eval()\n            with torch.no_grad():\n                action_values = self.q_network(observation)\n            # Back to training mode\n            self.q_network.train()\n            action = np.argmax(action_values.cpu().data.numpy())\n            return action    \n        \n    def checkpoint(self, filename):\n        torch.save(self.q_network.state_dict(), filename)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:16:43.258074Z","iopub.execute_input":"2024-10-13T20:16:43.258669Z","iopub.status.idle":"2024-10-13T20:16:43.280985Z","shell.execute_reply.started":"2024-10-13T20:16:43.258635Z","shell.execute_reply":"2024-10-13T20:16:43.279721Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_EPISODES = 3000  # Max number of episodes to play\nMAX_STEPS = 1000     # Max steps allowed in a single episode/play\nENV_SOLVED = 200     # MAX score at which we consider environment to be solved\nPRINT_EVERY = 100    # How often to print the progress\n\n\nEPS_START = 1.0      # Default/starting value of eps\nEPS_DECAY = 0.999    # Epsilon decay rate\nEPS_MIN = 0.01    ","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:16:43.282347Z","iopub.execute_input":"2024-10-13T20:16:43.282717Z","iopub.status.idle":"2024-10-13T20:16:43.292951Z","shell.execute_reply.started":"2024-10-13T20:16:43.282676Z","shell.execute_reply":"2024-10-13T20:16:43.292216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"EPS_DECAY_RATES = [0.9, 0.99, 0.999, 0.9999]\nplt.figure(figsize=(10,6))\n\nfor decay_rate in EPS_DECAY_RATES:\n    test_eps = EPS_START\n    eps_list = []\n    for _ in range(MAX_EPISODES):\n        test_eps = max(test_eps * decay_rate, EPS_MIN)\n        eps_list.append(test_eps)          \n    \n    plt.plot(eps_list, label='decay rate: {}'.format(decay_rate))\n\nplt.title('Effect of various decay rates')\nplt.legend(loc='best')\nplt.xlabel('# of episodes')\nplt.ylabel('epsilon')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:16:43.294055Z","iopub.execute_input":"2024-10-13T20:16:43.294354Z","iopub.status.idle":"2024-10-13T20:16:43.625584Z","shell.execute_reply.started":"2024-10-13T20:16:43.294323Z","shell.execute_reply":"2024-10-13T20:16:43.624253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state_size = env.observation_space.shape[0]\naction_size = env.action_space.n\n\nprint('State size: {}, action size: {}'.format(state_size, action_size))","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:16:43.629759Z","iopub.execute_input":"2024-10-13T20:16:43.630038Z","iopub.status.idle":"2024-10-13T20:16:43.634716Z","shell.execute_reply.started":"2024-10-13T20:16:43.630008Z","shell.execute_reply":"2024-10-13T20:16:43.633856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dqn_agent = DQNAgent(state_size, action_size, seed=0)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:16:43.635979Z","iopub.execute_input":"2024-10-13T20:16:43.636282Z","iopub.status.idle":"2024-10-13T20:16:43.662596Z","shell.execute_reply.started":"2024-10-13T20:16:43.636251Z","shell.execute_reply":"2024-10-13T20:16:43.661931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dqn_scores = []\n# Maintain a list of last 100 scores\ndqn_scores_window = deque(maxlen=100)\neps = EPS_START\nstart = time()\ndqn_actions = []\nfor episode in range(1, MAX_EPISODES + 1):\n    state = env.reset(seed = 42)\n    score = 0\n    episode_actions = []\n    for t in range(MAX_STEPS):\n        action = dqn_agent.act(state, eps)  \n        episode_actions.append(action)\n\n        #note that observation is the state after taking the action\n        observation, reward, terminated, truncated, info = env.step(action)\n        if isinstance(observation, tuple):\n            observation = observation[0]\n            \n        dqn_agent.step(state, action, reward, observation, terminated)\n        state = observation        \n        score += reward        \n        if terminated:\n            break\n            \n        eps = max(eps * EPS_DECAY, EPS_MIN)\n        mean_score = 0\n        if episode % PRINT_EVERY == 0:\n            mean_score = np.mean(dqn_scores_window)\n            print('\\r Progress {}/{}, average score:{:.2f}'.format(episode, MAX_EPISODES, mean_score), end=\"\")\n        if mean_score >= ENV_SOLVED:\n            print('\\rEnvironment solved in {} episodes, average score: {:.2f}'.format(episode, mean_score), end=\"\")\n            sys.stdout.flush()\n            dqn_agent.checkpoint('solved_200.pth')\n            break\n            \n    dqn_scores_window.append(score)\n    dqn_scores.append(score)\n    dqn_actions.append(episode_actions)\n    \nend = time()    \nprint('Took {} seconds'.format(end - start))\ndqn_time = end-start","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:16:43.666609Z","iopub.execute_input":"2024-10-13T20:16:43.666905Z","iopub.status.idle":"2024-10-13T20:44:04.721045Z","shell.execute_reply.started":"2024-10-13T20:16:43.666856Z","shell.execute_reply":"2024-10-13T20:44:04.719853Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.close()\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:44:04.722527Z","iopub.execute_input":"2024-10-13T20:44:04.722940Z","iopub.status.idle":"2024-10-13T20:44:04.728600Z","shell.execute_reply.started":"2024-10-13T20:44:04.722877Z","shell.execute_reply":"2024-10-13T20:44:04.727401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Actor Network\nclass Actor(nn.Module):\n    def __init__(self, state_dim, action_dim, hidden_dim=256):\n        super(Actor, self).__init__()\n        self.fc1 = nn.Linear(state_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.action_head = nn.Linear(hidden_dim, action_dim)\n\n    def forward(self, state):\n        x = torch.tanh(self.fc1(state))\n        x = torch.tanh(self.fc2(x))\n        action_probs = torch.softmax(self.action_head(x), dim=-1)\n        return action_probs\n\n# Critic Network\nclass Critic(nn.Module):\n    def __init__(self, state_dim, hidden_dim=256):\n        super(Critic, self).__init__()\n        self.fc1 = nn.Linear(state_dim, hidden_dim)\n        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n        self.value_head = nn.Linear(hidden_dim, 1)\n\n    def forward(self, state):\n        x = torch.tanh(self.fc1(state))\n        x = torch.tanh(self.fc2(x))\n        state_value = self.value_head(x)\n        return state_value\n\n# Memory for storing experience\nclass Memory:\n    def __init__(self):\n        self.states = []\n        self.actions = []\n        self.logprobs = []\n        self.rewards = []\n        self.is_terminals = []\n\n    def clear(self):\n        del self.states[:]\n        del self.actions[:]\n        del self.logprobs[:]\n        del self.rewards[:]\n        del self.is_terminals[:]\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:44:04.730335Z","iopub.execute_input":"2024-10-13T20:44:04.730717Z","iopub.status.idle":"2024-10-13T20:44:04.760468Z","shell.execute_reply.started":"2024-10-13T20:44:04.730674Z","shell.execute_reply":"2024-10-13T20:44:04.759400Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PPOAgent:\n    def __init__(self, state_dim, action_dim, lr=0.0003, gamma=0.99, K_epochs=4, eps_clip=0.2):\n        self.gamma = gamma\n        self.eps_clip = eps_clip\n        self.K_epochs = K_epochs\n\n        self.actor = Actor(state_dim, action_dim)\n        self.critic = Critic(state_dim)\n\n        self.optimizer = optim.Adam([\n            {'params': self.actor.parameters(), 'lr': lr},\n            {'params': self.critic.parameters(), 'lr': lr}\n        ])\n\n        self.policy_old = Actor(state_dim, action_dim)\n        self.policy_old.load_state_dict(self.actor.state_dict())\n\n        self.MseLoss = nn.MSELoss()\n\n    def select_action(self, state, memory):\n        state = torch.FloatTensor(state)\n        with torch.no_grad():\n            action_probs = self.policy_old(state)\n        dist = Categorical(action_probs)\n        action = dist.sample()\n\n        memory.states.append(state)\n        memory.actions.append(action)\n        memory.logprobs.append(dist.log_prob(action))\n        return action.item()\n\n    def update(self, memory):\n        # Convert list to tensor\n        old_states = torch.stack(memory.states).detach()\n        old_actions = torch.tensor(memory.actions).detach()\n        old_logprobs = torch.stack(memory.logprobs).detach()\n\n        # Compute discounted rewards\n        rewards = []\n        discounted_reward = 0\n        for reward, is_terminal in zip(reversed(memory.rewards), reversed(memory.is_terminals)):\n            if is_terminal:\n                discounted_reward = 0\n            discounted_reward = reward + (self.gamma * discounted_reward)\n            rewards.insert(0, discounted_reward)\n        rewards = torch.tensor(rewards, dtype=torch.float32).detach()\n        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-5)\n\n        # Optimize policy for K epochs\n        for _ in range(self.K_epochs):\n            # Evaluate old actions and values\n            action_probs = self.actor(old_states)\n            dist = Categorical(action_probs)\n\n            # New log probabilities and state values\n            logprobs = dist.log_prob(old_actions)\n            state_values = self.critic(old_states).squeeze()\n            dist_entropy = dist.entropy()\n\n            # Ratios for PPO\n            ratios = torch.exp(logprobs - old_logprobs)\n\n            # Advantages\n            advantages = rewards - state_values.detach()\n\n            # Surrogate loss\n            surr1 = ratios * advantages\n            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n\n            # Loss function\n            loss = -torch.min(surr1, surr2) + 0.5 * self.MseLoss(state_values, rewards) - 0.01 * dist_entropy\n\n            # Take gradient step\n            self.optimizer.zero_grad()\n            loss.mean().backward()\n            self.optimizer.step()\n\n        # Copy new weights into old policy\n        self.policy_old.load_state_dict(self.actor.state_dict())","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:44:04.762060Z","iopub.execute_input":"2024-10-13T20:44:04.762556Z","iopub.status.idle":"2024-10-13T20:44:04.784328Z","shell.execute_reply.started":"2024-10-13T20:44:04.762521Z","shell.execute_reply":"2024-10-13T20:44:04.782872Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = gym.make('LunarLander-v2')\n\n# Get the size of the state and action spaces\nstate_dim = env.observation_space.shape[0]\naction_dim = env.action_space.n\n\n# Instantiate the PPO agent\nppo = PPOAgent(state_dim, action_dim)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:44:04.785646Z","iopub.execute_input":"2024-10-13T20:44:04.786258Z","iopub.status.idle":"2024-10-13T20:44:04.799796Z","shell.execute_reply.started":"2024-10-13T20:44:04.786210Z","shell.execute_reply":"2024-10-13T20:44:04.798844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Parameters\nmax_episodes = 3000  # Number of episodes to train\nmax_timesteps = 1000  # Max timesteps per episode\nupdate_timestep = 1024  # Timesteps after which to update the policy\ntimestep = 0\n\n# To track scores\nppo_scores = []  # Store total reward per episode\nscores_window_ppo = deque(maxlen=100)  # Last 100 episode scores for moving average\nppo_actions = []\nmemory = Memory()\n# Run training loop\nstart = time()\nfor episode in range(1, max_episodes + 1):\n    state, _ = env.reset()\n    total_reward = 0\n    episode_actions = []\n    for t in range(max_timesteps):\n        timestep += 1\n        action = ppo.select_action(state, memory)\n        episode_actions.append(action)\n        state, reward, terminated, _, _ = env.step(action)\n\n        memory.rewards.append(reward)\n        memory.is_terminals.append(terminated)\n        total_reward += reward\n\n        # Update PPO after reaching update timestep\n        if timestep % update_timestep == 0:\n            ppo.update(memory)\n            memory.clear()\n            timestep = 0\n\n        if terminated:\n            break\n\n    # Store the score for the current episode\n    ppo_scores.append(total_reward)\n    scores_window_ppo.append(total_reward)  # Save the most recent score\n    ppo_actions.append(episode_actions)\n    \n    mean_score = 0\n    if episode % PRINT_EVERY == 0:\n        mean_score = np.mean(scores_window_ppo)\n        print('\\r Progress {}/{}, average score:{:.2f}'.format(episode, MAX_EPISODES, mean_score), end=\"\")\n    if mean_score >= ENV_SOLVED:\n        print('\\rEnvironment solved in {} episodes, average score: {:.2f}'.format(episode, mean_score), end=\"\")\n        sys.stdout.flush()\n        break\n\nend = time()    \nprint('Took {} seconds'.format(end - start))\nppo_time = end-start\n\nenv.close()","metadata":{"execution":{"iopub.status.busy":"2024-10-13T20:44:04.801065Z","iopub.execute_input":"2024-10-13T20:44:04.801434Z","iopub.status.idle":"2024-10-13T21:01:56.836191Z","shell.execute_reply.started":"2024-10-13T20:44:04.801391Z","shell.execute_reply":"2024-10-13T21:01:56.835154Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.plot(dqn_scores)\nplt.plot(pd.Series(dqn_scores).rolling(100).mean())\nplt.title('DQN Training for Lunar Landing')\nplt.xlabel('# of episodes')\nplt.ylabel('score')\nplt.savefig('dqn_lunar.png', dpi=300)\n\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T21:01:56.837516Z","iopub.execute_input":"2024-10-13T21:01:56.837925Z","iopub.status.idle":"2024-10-13T21:01:57.615977Z","shell.execute_reply.started":"2024-10-13T21:01:56.837859Z","shell.execute_reply":"2024-10-13T21:01:57.615060Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.plot(ppo_scores)\nplt.plot(pd.Series(ppo_scores).rolling(100).mean())\nplt.title('PPO Training for Lunar Landing')\nplt.xlabel('# of episodes')\nplt.ylabel('score')\nplt.savefig('ppo_lunar.png', dpi=300)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-13T21:01:57.617150Z","iopub.execute_input":"2024-10-13T21:01:57.617442Z","iopub.status.idle":"2024-10-13T21:01:58.463188Z","shell.execute_reply.started":"2024-10-13T21:01:57.617410Z","shell.execute_reply":"2024-10-13T21:01:58.462253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_rewards(dqn_rewards, ppo_rewards):\n    plt.plot(dqn_rewards, label='DQN')\n    plt.plot(ppo_rewards, label='PPO')\n    plt.xlabel('Episodes')\n    plt.ylabel('Cumulative Reward')\n    plt.title('Reward Comparison: DQN vs PPO')\n    plt.legend()\n    plt.show()\n\nplot_rewards(dqn_scores, ppo_scores)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T21:01:58.464632Z","iopub.execute_input":"2024-10-13T21:01:58.465017Z","iopub.status.idle":"2024-10-13T21:01:58.702765Z","shell.execute_reply.started":"2024-10-13T21:01:58.464965Z","shell.execute_reply":"2024-10-13T21:01:58.701927Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_reward_variance(dqn_rewards, ppo_rewards, window=100):\n    dqn_variance = [np.var(dqn_rewards[max(0, i-window):i+1]) for i in range(len(dqn_rewards))]\n    ppo_variance = [np.var(ppo_rewards[max(0, i-window):i+1]) for i in range(len(ppo_rewards))]\n    \n    plt.plot(dqn_variance, label='DQN Reward Variance')\n    plt.plot(ppo_variance, label='PPO Reward Variance')\n    plt.xlabel('Episodes')\n    plt.ylabel('Variance in Rewards')\n    plt.title('Reward Variance: DQN vs PPO')\n    plt.legend()\n    plt.show()\n\nplot_reward_variance(dqn_scores, ppo_scores)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T21:01:58.704078Z","iopub.execute_input":"2024-10-13T21:01:58.704383Z","iopub.status.idle":"2024-10-13T21:01:59.161082Z","shell.execute_reply.started":"2024-10-13T21:01:58.704351Z","shell.execute_reply":"2024-10-13T21:01:59.160169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nplt.figure(figsize=(10, 6))\nplt.plot(pd.Series(ppo_scores).rolling(100).mean(), label=\"ppo\")\nplt.plot(pd.Series(dqn_scores).rolling(100).mean(), label=\"dqn\")\n\nplt.title('DQN vs PPO scores averaged per 100 episodes for Lunar Landing')\nplt.xlabel('# of episodes')\nplt.ylabel('score')\n\n# Get the last points of the PPO and DQN lines for labeling\nppo_label_x = len(ppo_scores)\nppo_label_y = pd.Series(ppo_scores).rolling(100).mean().iloc[-1]\n\ndqn_label_x = len(dqn_scores)\ndqn_label_y = pd.Series(dqn_scores).rolling(100).mean().iloc[-1]\n\n# Place labels near the end of the plot lines\nplt.text(ppo_label_x - 50, ppo_label_y, 'PPO', color='blue', fontsize=12, fontweight='bold')\nplt.text(dqn_label_x - 50, dqn_label_y, 'DQN', color='orange', fontsize=12, fontweight='bold')\n\nplt.legend()\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T21:01:59.162208Z","iopub.execute_input":"2024-10-13T21:01:59.162522Z","iopub.status.idle":"2024-10-13T21:01:59.467497Z","shell.execute_reply.started":"2024-10-13T21:01:59.162489Z","shell.execute_reply":"2024-10-13T21:01:59.466656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dqn_flattened_actions = [action for episode in dqn_actions for action in episode]\nppo_flattened_actions = [action for episode in ppo_actions for action in episode]\n\ndef plot_action_distribution(dqn_actions, ppo_actions):\n    dqn_action_counts = [dqn_actions.count(i) for i in range(4)]  # Assuming 4 actions (0-3)\n    ppo_action_counts = [ppo_actions.count(i) for i in range(4)]\n    \n    plt.bar(range(4), dqn_action_counts, alpha=0.6, label='DQN', width=0.4)\n    plt.bar([i + 0.4 for i in range(4)], ppo_action_counts, alpha=0.6, label='PPO', width=0.4)\n    plt.xlabel('Actions')\n    plt.ylabel('Frequency')\n    plt.title('Action Distribution: DQN vs PPO')\n    plt.legend()\n    plt.show()\n\nplot_action_distribution(dqn_flattened_actions, ppo_flattened_actions)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T21:01:59.468678Z","iopub.execute_input":"2024-10-13T21:01:59.468995Z","iopub.status.idle":"2024-10-13T21:02:00.415538Z","shell.execute_reply.started":"2024-10-13T21:01:59.468962Z","shell.execute_reply":"2024-10-13T21:02:00.414629Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Time taken by DQN for 3000 episodes : {dqn_time}\")\nprint(f\"Time taken by PPO for 3000 episodes : {ppo_time}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T21:36:26.257306Z","iopub.execute_input":"2024-10-13T21:36:26.257971Z","iopub.status.idle":"2024-10-13T21:36:26.263043Z","shell.execute_reply.started":"2024-10-13T21:36:26.257930Z","shell.execute_reply":"2024-10-13T21:36:26.262058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.close()","metadata":{"execution":{"iopub.status.busy":"2024-10-13T21:02:00.423803Z","iopub.execute_input":"2024-10-13T21:02:00.424119Z","iopub.status.idle":"2024-10-13T21:02:00.430488Z","shell.execute_reply.started":"2024-10-13T21:02:00.424087Z","shell.execute_reply":"2024-10-13T21:02:00.429672Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#new environment\n\nenv = gym.make(\"CartPole-v1\")","metadata":{"execution":{"iopub.status.busy":"2024-10-13T21:02:00.431547Z","iopub.execute_input":"2024-10-13T21:02:00.431846Z","iopub.status.idle":"2024-10-13T21:02:00.440677Z","shell.execute_reply.started":"2024-10-13T21:02:00.431801Z","shell.execute_reply":"2024-10-13T21:02:00.439770Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MAX_EPISODES = 1000  # Max number of episodes to play\nMAX_STEPS = 1000     # Max steps allowed in a single episode/play\nENV_SOLVED = 195     # MAX score at which we consider environment to be solved\nPRINT_EVERY = 100    # How often to print the progress\n\n\nEPS_START = 1.0      # Default/starting value of eps\nEPS_DECAY = 0.999    # Epsilon decay rate\nEPS_MIN = 0.01","metadata":{"execution":{"iopub.status.busy":"2024-10-13T21:02:00.441832Z","iopub.execute_input":"2024-10-13T21:02:00.442204Z","iopub.status.idle":"2024-10-13T21:02:00.450151Z","shell.execute_reply.started":"2024-10-13T21:02:00.442163Z","shell.execute_reply":"2024-10-13T21:02:00.449307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"state_size = env.observation_space.shape[0]\naction_size = env.action_space.n\n\nprint('State size for CartPole : {}, action size: {}'.format(state_size, action_size))\n\ndqn_agent = DQNAgent(state_size, action_size, seed=0)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T21:02:00.455137Z","iopub.execute_input":"2024-10-13T21:02:00.455409Z","iopub.status.idle":"2024-10-13T21:02:00.502487Z","shell.execute_reply.started":"2024-10-13T21:02:00.455380Z","shell.execute_reply":"2024-10-13T21:02:00.501677Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#DQN for CartPole\n\ndqn_scores = []\n# Maintain a list of last 100 scores\ndqn_scores_window = deque(maxlen=100)\neps = EPS_START\nstart = time()\ndqn_actions = []\nfor episode in range(1, MAX_EPISODES + 1):\n    state = env.reset(seed = 42)\n    score = 0\n    episode_actions = []\n    for t in range(MAX_STEPS):\n        action = dqn_agent.act(state, eps)  \n        episode_actions.append(action)\n\n        #note that observation is the state after taking the action\n        observation, reward, terminated, truncated, info = env.step(action)\n        if isinstance(observation, tuple):\n            observation = observation[0]\n            \n        dqn_agent.step(state, action, reward, observation, terminated)\n        state = observation        \n        score += reward        \n        if terminated:\n            break\n            \n        eps = max(eps * EPS_DECAY, EPS_MIN)\n        mean_score = 0\n        if episode % PRINT_EVERY == 0:\n            mean_score = np.mean(dqn_scores_window)\n            print('\\r Progress {}/{}, average score:{:.2f}'.format(episode, MAX_EPISODES, mean_score), end=\"\")\n        if mean_score >= ENV_SOLVED:\n            print('\\rEnvironment solved in {} episodes, average score: {:.2f}'.format(episode, mean_score), end=\"\")\n            sys.stdout.flush()\n            dqn_agent.checkpoint('solved_200.pth')\n            break\n            \n    dqn_scores_window.append(score)\n    dqn_scores.append(score)\n    dqn_actions.append(episode_actions)\n    \nend = time()    \nprint('Took {} seconds'.format(end - start))\ndqn_time = end-start","metadata":{"execution":{"iopub.status.busy":"2024-10-13T21:02:00.521708Z","iopub.execute_input":"2024-10-13T21:02:00.522022Z","iopub.status.idle":"2024-10-13T21:03:41.350940Z","shell.execute_reply.started":"2024-10-13T21:02:00.521990Z","shell.execute_reply":"2024-10-13T21:03:41.349914Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env.close()","metadata":{"execution":{"iopub.status.busy":"2024-10-13T21:03:41.352279Z","iopub.execute_input":"2024-10-13T21:03:41.352678Z","iopub.status.idle":"2024-10-13T21:03:41.357557Z","shell.execute_reply.started":"2024-10-13T21:03:41.352632Z","shell.execute_reply":"2024-10-13T21:03:41.356525Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"env = gym.make('CartPole-v1')\n\n# Get the size of the state and action spaces\nstate_dim = env.observation_space.shape[0]\naction_dim = env.action_space.n\n\n# Instantiate the PPO agent\nppo = PPOAgent(state_dim, action_dim)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T21:03:41.358851Z","iopub.execute_input":"2024-10-13T21:03:41.359195Z","iopub.status.idle":"2024-10-13T21:03:41.373118Z","shell.execute_reply.started":"2024-10-13T21:03:41.359162Z","shell.execute_reply":"2024-10-13T21:03:41.372156Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Parameters\nmax_episodes = 1000  # Number of episodes to train\nmax_timesteps = 300  # Max timesteps per episode\nupdate_timestep = 500  # Timesteps after which to update the policy\ntimestep = 0\n\n# To track scores\nppo_scores = []  # Store total reward per episode\nscores_window_ppo = deque(maxlen=100)  # Last 100 episode scores for moving average\nppo_actions = []\nmemory = Memory()\n# Run training loop\nstart = time()\nfor episode in range(1, max_episodes + 1):\n    state, _ = env.reset()\n    total_reward = 0\n    episode_actions = []\n    for t in range(max_timesteps):\n        timestep += 1\n        action = ppo.select_action(state, memory)\n        episode_actions.append(action)\n        state, reward, terminated, _, _ = env.step(action)\n\n        memory.rewards.append(reward)\n        memory.is_terminals.append(terminated)\n        total_reward += reward\n\n        # Update PPO after reaching update timestep\n        if timestep % update_timestep == 0:\n            ppo.update(memory)\n            memory.clear()\n            timestep = 0\n\n        if terminated:\n            break\n\n    # Store the score for the current episode\n    ppo_scores.append(total_reward)\n    scores_window_ppo.append(total_reward)  # Save the most recent score\n    ppo_actions.append(episode_actions)\n    \n    mean_score = 0\n    if episode % PRINT_EVERY == 0:\n        mean_score = np.mean(scores_window_ppo)\n        print('\\r Progress {}/{}, average score:{:.2f}'.format(episode, MAX_EPISODES, mean_score), end=\"\")\n    if mean_score >= ENV_SOLVED:\n        print('\\rEnvironment solved in {} episodes, average score: {:.2f}'.format(episode, mean_score), end=\"\")\n        sys.stdout.flush()\n        break\n        \nend = time()    \nprint('Took {} seconds'.format(end - start))\nppo_time = end-start\n\nenv.close()","metadata":{"execution":{"iopub.status.busy":"2024-10-13T21:03:41.374397Z","iopub.execute_input":"2024-10-13T21:03:41.374710Z","iopub.status.idle":"2024-10-13T21:04:03.067982Z","shell.execute_reply.started":"2024-10-13T21:03:41.374679Z","shell.execute_reply":"2024-10-13T21:04:03.067038Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.plot(dqn_scores)\nplt.plot(pd.Series(dqn_scores).rolling(100).mean())\nplt.title('DQN Training for CartPole')\nplt.xlabel('# of episodes')\nplt.ylabel('score')\nplt.savefig('dqn_Cart_Pole.png', dpi=300)\n\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T21:04:03.069271Z","iopub.execute_input":"2024-10-13T21:04:03.069583Z","iopub.status.idle":"2024-10-13T21:04:03.891229Z","shell.execute_reply.started":"2024-10-13T21:04:03.069549Z","shell.execute_reply":"2024-10-13T21:04:03.890354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10,6))\nplt.plot(ppo_scores)\nplt.plot(pd.Series(ppo_scores).rolling(100).mean())\nplt.title('PPO Training for cart pole')\nplt.xlabel('# of episodes')\nplt.ylabel('score')\nplt.savefig('ppo_cart_pole.png', dpi=300)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-13T21:04:03.892574Z","iopub.execute_input":"2024-10-13T21:04:03.893170Z","iopub.status.idle":"2024-10-13T21:04:04.736748Z","shell.execute_reply.started":"2024-10-13T21:04:03.893122Z","shell.execute_reply":"2024-10-13T21:04:04.735840Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def plot_reward_variance(dqn_rewards, ppo_rewards, window=100):\n    dqn_variance = [np.var(dqn_rewards[max(0, i-window):i+1]) for i in range(len(dqn_rewards))]\n    ppo_variance = [np.var(ppo_rewards[max(0, i-window):i+1]) for i in range(len(ppo_rewards))]\n    \n    plt.plot(dqn_variance, label='DQN Reward Variance')\n    plt.plot(ppo_variance, label='PPO Reward Variance')\n    plt.xlabel('Episodes')\n    plt.ylabel('Variance in Rewards')\n    plt.title('Reward Variance: DQN vs PPO')\n    plt.legend()\n    plt.show()\n\nplot_reward_variance(dqn_scores, ppo_scores)","metadata":{"execution":{"iopub.status.busy":"2024-10-13T21:04:04.738060Z","iopub.execute_input":"2024-10-13T21:04:04.738408Z","iopub.status.idle":"2024-10-13T21:04:05.105356Z","shell.execute_reply.started":"2024-10-13T21:04:04.738368Z","shell.execute_reply":"2024-10-13T21:04:05.104454Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dqn_flattened_actions = [action for episode in dqn_actions for action in episode]\nppo_flattened_actions = [action for episode in ppo_actions for action in episode]\n\ndef plot_action_distribution(dqn_actions, ppo_actions):\n    dqn_action_counts = [dqn_actions.count(i) for i in range(2)]  # CartPole has 2 actions (0 and 1)\n    ppo_action_counts = [ppo_actions.count(i) for i in range(2)]\n    \n    plt.bar(range(2), dqn_action_counts, alpha=0.6, label='DQN', width=0.4, align='center')\n    plt.bar([i + 0.4 for i in range(2)], ppo_action_counts, alpha=0.6, label='PPO', width=0.4, align='center')\n    plt.xlabel('Actions (0: Left, 1: Right)')\n    plt.ylabel('Frequency')\n    plt.title('Action Distribution: DQN vs PPO on CartPole')\n    plt.xticks([0.2, 1.2], ['Left (0)', 'Right (1)'])  # Adjusted to show labels for both actions\n    plt.legend()\n    plt.show()\n\nplot_action_distribution(dqn_flattened_actions, ppo_flattened_actions)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T21:04:05.106608Z","iopub.execute_input":"2024-10-13T21:04:05.106942Z","iopub.status.idle":"2024-10-13T21:04:05.426525Z","shell.execute_reply.started":"2024-10-13T21:04:05.106901Z","shell.execute_reply":"2024-10-13T21:04:05.425588Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Time taken by DQN for solving env : {dqn_time}\")\nprint(f\"Time taken by PPO for solving env : {ppo_time}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-13T21:35:42.337809Z","iopub.execute_input":"2024-10-13T21:35:42.338200Z","iopub.status.idle":"2024-10-13T21:35:42.343325Z","shell.execute_reply.started":"2024-10-13T21:35:42.338165Z","shell.execute_reply":"2024-10-13T21:35:42.342359Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Environment - Lunar Landing V2\n\n**DQN vs PPO training performance** \n\nDQN :\n\n* DQN shows significant instability before episode 1000. However, after that, the performance stabilizes, and DQN achieves rewards above 200 after ~1200 episodes.\n* By the end of training, DQN comes close in solving the environment.\n\nPPO : \n\n* PPO demonstrates more consistent learning in the early episodes, achieving steady improvement up until around 1000 episodes.\n* Despite reaching a decent level of performance, PPO does not reach the same level of reward as DQN by the end of training.\n\n**Reward Variance**\n\n* DQN has a much higher variance compared to PPO and thus, is unstable compared to PPO. PPO suggests a stable learning process.\n\n**Average scores over time**\n\n* DQN has better long-term performance, though PPO is better early on in training.\n\n**Action Distribution**\n\n* DQN shows a preference for taking action 2, while PPO distributes more evenly.\n* DQN is more exploitative of certain actions, while PPO maintains broader exploration\n\n**Conclusion**\n* DQN is more performance-efficient, with it reaching higher rewards, but it is also much higher in variance and volatility. It is less sample-efficient early in training but surpasses PPO after enough episodes.\n* PPO is more sample-efficient early, stabilizes faster, and its reward variance is much lower. However, it converges to a lower score compared to DQN overall. \n* PPO is much more hyperparameter sensitive thus some more hyperparameters tuning is needed.\n\n","metadata":{}},{"cell_type":"markdown","source":"# Environment - Cart Pole\n\n**DQN vs PPO training performance** \n\n* DQN starts slowly, with low rewards for the first 400-500 episodes. Performance sharply improves around episode 600, nearing a score of 200 by episode 1000. However, performance dips toward the end, stabilizing just below 200.\n* PPO performs lightning fast solving the environment in 300 episodes\n\n**Reward Variance**\n\n* PPO demonstrates better consistency with much lower variance in rewards, while DQN exhibits large fluctuations throughout training, reflecting its instability.\n\n**Action Distribution**\n\n* Both seem to have an equal split in their choice of actions though DQN does favour action 1 slightly more.\n\n**Conclusion**\n* PPO outperforms DQN in the CartPole environment in terms of both sample efficiency and performance efficiency, solving the environment faster and with greater stability.\n","metadata":{}},{"cell_type":"markdown","source":"***Final*** -\n\nMy implementation of DQN makes use of a fully connected neural network instead of a CNN as suggested in the paper based on this implementation - \n\nhttps://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n\nMy PPO implementation was inspired from several sites and my own attempt at interpreting the algorithm. The results clearly demonstrate a need for more experimentation with hyperparameters considering its sensitivity in the PPO agent.\n\nOverall, the PPO agent was consistently less time taking completing the same number of episodes DQN runs for but in a lesser time. \n\n","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}